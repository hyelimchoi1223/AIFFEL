{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infinite-spanking",
   "metadata": {},
   "source": [
    "추론을 기반으로 한 단어의 분산표현. 추론 과정에서 신경망을 이용하는데 여기서 word2vec이 등장한다.\n",
    "\n",
    "## 이번장의 목표 : 간단한 word2vec 구현\n",
    "\n",
    "# 3.1 추론기반 기법과 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-cooperative",
   "metadata": {},
   "source": [
    "## 3.1.1 통계 기반 기법의 문제점\n",
    "\n",
    "통계 기반 기법은 주변 단어의 빈도를 가지고 단어를 표현했다. (동시 발생 행렬을 만들고, SVD를 적용해 단어의 분산 표현을 얻음) 이 방식은 대규모 말뭉치를 다룰 때 문제가 된다.\n",
    "\n",
    "* 통계 기반 기법 : 전체 통계를 이용해 한번의 계산과정으로 단어의 분산 표현을 얻는다.\n",
    "* 추론 기반 기법 : 미니 배치 학습을 이용해 순차적으로 학습한다.\n",
    "=> 계산량이 클 때도 데이터를 작게 나눠 학습시키기 때문에 신경망을 학습시킬 수 있다. \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134807641-fa6784d5-67af-476e-9938-641c0c38f6a9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-oxford",
   "metadata": {},
   "source": [
    "## 3.1.2 추론 기반 기법 개요\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134807686-e3b40da7-e497-487e-86e4-02b806354c07.png)\n",
    "\n",
    "주변 단어를 이용해 \"?\"에 들어갈 단어를 추측하는 것. => 추론 문제를 풀고 학습하는 것이 \"추론 기반 기법\" 이다.    \n",
    "'모델관점'에서 보면 다음 그림처럼 보인다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134807726-6ee936ae-acb4-49b8-bae6-9cc4eaf7e87b.png)\n",
    "\n",
    "추론 기반 기법에서는 어떤 모델이 등장하는데 이 모델로 신경망을 사용한다.    \n",
    "모델은 단어의 출현 확률을 출력하는데 올바른 추측을 하도록 훈련시킨다. 그리고 그 학습 결과로 단어의 분산 표현을 얻는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-single",
   "metadata": {},
   "source": [
    "## 3.1.3 신경망에서의 단어 처리\n",
    "\n",
    "신경망을 이용해 '단어'를 처리해야 하는데 'you', 'say' 같은 단어를 있는 그대로 처리할 수 없으므로 이런 단어를 '고정 길이의 벡터'로 변환해야 한다. 이때 사용하는 대표적인 방법이 one-hot-vector로 표현하는 것이다. \n",
    "> ### one-hot-vector    \n",
    "> 벡터 원소 중 한개만 1이고 나머지는 0\n",
    "> ![image](https://user-images.githubusercontent.com/63278762/134807911-0b874489-36ae-4319-a273-bf03f9fa29a3.png)\n",
    "> \n",
    "> 1. 총 어휘 수만큼의 원소를 갖는 벡터를 준비\n",
    "> 2. 인덱스가 단어 ID와 같은 원소를 1로, 나머지는 0으로 설정\n",
    "\n",
    "\n",
    "=> 단어를 고정 길이 벡터로 변환하면 신경망의 입력층의 뉴런 수를 고정할 수 있다.\n",
    "![image](https://user-images.githubusercontent.com/63278762/134807989-01c98787-fab3-49e0-b655-ba45921fc2b9.png)\n",
    "\n",
    "단어를 벡터로 나타내었으니 신경망으로 처리할 수 있다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134808122-c3d18575-49c0-4af3-a9f6-d1888e042972.png)\n",
    "\n",
    "위의 예는 완전연결계층이므로 모든 노드가 화살표로 연결되어있다. 화살표에는 가중치가 있고, 입력층 뉴런과 가중합이 은닉층 뉴런이 된다. (여기서는 완전연결계층에서의 편향을 생략했다.)    \n",
    "> 편향이 없는 완전연결계층은 MatMul 계층처럼 행렬 곱 계산에 해당된다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134808227-46e6322d-24a2-4895-89bd-36f6b5acd97d.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adult-gather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97457071 0.43642852 0.40606851]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([[1,0,0,0,0,0,0]]) # 입력\n",
    "W = np.random.rand(7,3) # 가중치\n",
    "h = np.matmul(c,W)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-british",
   "metadata": {},
   "source": [
    "완전연결계층은 matmul을 이용해 계산하였다.(이번 예에서는 편향을 생략하였기 때문에)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "experienced-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common.layers\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appreciated-electric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97324113 0.5153196  0.55397412]]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([[1,0,0,0,0,0,0]]) # 입력\n",
    "W = np.random.rand(7,3) # 가중치\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-oasis",
   "metadata": {},
   "source": [
    "## 3.2 단순한 word2vec\n",
    "추론하는 모델을 신경망으로 구축해본다. 이번 절에서는 word2vec에서 제안하는 CBOW(continuous bag-of-words)모델이다. \n",
    "> word2vec에서 사용하는 신경망은 CBOW, skip-gram 모델이 있다.\n",
    "\n",
    "### 3.2.1 CBOW 모델의 추론 처리\n",
    "CBOW : 맥락으로부터 타깃을 추측하는 신경망\n",
    "* 맥락: 주변 단어\n",
    "* 타겟: 중앙 단어\n",
    "목표는 CBOW 모델이 정확하게 추론하도록 훈련시켜 단어의 분산 표현을 얻어내는 것.\n",
    "\n",
    "CBOW 모델의 입력은 맥락이다. 가장 먼저 맥락을 원핫 표현으로 변환해 CBOW 모델이 처리할 수 있도록 준비한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134808760-defd7d55-16ce-4a6a-b024-346aca1783a4.png)\n",
    "\n",
    "> 입력층이 2개인 이유 : 맥락으로 고려할 단어를 2개로 정했기 때문이다. \n",
    "\n",
    "두 입력층->은닉층 변환 : 똑같은 완전연결계층이 처리\n",
    "은닉층-> 출력층 변환 : 다른 완전연결계층이 처리\n",
    "\n",
    "#### 은닉층\n",
    "입력층이 여러개이면 전체를 평균한 것이 은닉층의 뉴런이 된다.\n",
    "첫번째 입력이 $h_1$으로 변환되고, 두번째 입력층이 $h_2$로 변환된다고 했을 때 은닉층 뉴런은 $\\frac{1}{2}(h_1+h_2)$가 된다.\n",
    "\n",
    "#### 출력층\n",
    "출력층의 뉴런 하나하나가 각각의 단어에 대응한다. 그리고 출력층 뉴런은 각 단어의 점수를 뜻하고, 값이 높을수록 대응 단어의 출현 확률도 높아진다. 여기서 점수는 확률로 해석되기 전의 값. 점수에 소프트맥스 함수를 적용해 확률를 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-humanitarian",
   "metadata": {},
   "source": [
    "> 은닉층의 뉴런 수를 입력층의 뉴런 수보다 적게 하는 것이 핵심이다. 이렇게 해야 은닉층에는 단어 예측에 필요한 정보를 간결하게 갖게 되고, 밀집벡터 표현을 얻을 수 있다. 이때 은닉층의 정보는 사람이 이해할 수 없는 코드로 되어있으므로 인코딩에 해당하고, 은닉층 정보로부터 원하는 결과를 얻는 작업을 디코딩이라고 한다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134809146-1ed3c896-4349-4cdf-ad00-df52cda94b84.png)\n",
    "\n",
    "위의 그림에서 0.5를 곱하는 계층이 있는데 이것은 입력층의 결과를 더해서 평균을 낸 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "biological-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33463339 -0.46591049 -0.37452352  0.05007738  0.01762296 -0.20009767\n",
      "   0.90480753]]\n"
     ]
    }
   ],
   "source": [
    "c0 = np.array([[1,0,0,0,0,0,0]])\n",
    "c1 = np.array([[0,0,1,0,0,0,0]])\n",
    "\n",
    "# 필요한 가중치들을 초기화\n",
    "W_in = np.random.randn(7,3)\n",
    "W_out = np.random.randn(3,7)\n",
    "\n",
    "# MatMul 계층을 생성\n",
    "# 입력층의 MatMul은 가중치를 공유한다.\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 순전파\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5*(h0+h1)\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-providence",
   "metadata": {},
   "source": [
    "## 3.2.2 CBOW 모델의 학습\n",
    "모델의 출력층에서 각 단어의 점수를 출력햇는데 이 점수를 소프트 맥스 함수를 적용해 확률를 얻을 수 있다.    \n",
    "여기서 이 확률은 각 단어가 출현할 확률을 나타낸다. \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134809397-da1a5544-850c-4990-83c2-664678ad65f9.png)\n",
    "\n",
    "CBOW 모델의 학습을 통해 올바른 예측을 할 수 있도록 가중치를 조정한다. 가중치 W_in에서는 단어의 출현 패턴을 파악한 벡터가 학습된다. \n",
    "> CBOW 모델은 단어 출현 패턴 학습 시 사용한 말뭉치로부터 배운다. 따라서 말뭉치가 다르면 얻게되는 단어 분산 표현도 달라진다.\n",
    "\n",
    "CBOW는 다중 클래스 분류를 수행하는 신경망이다. 따라서 이 신경망을 학습하려면 softmax와 crossentropy 오차만 이용하면 된다.    \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134809530-fd1b70ae-477a-4171-922e-9255bec77b22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-alignment",
   "metadata": {},
   "source": [
    "출력층에서 얻는 점수를 softmax를 통과하면서 확률로 변환하고, 그 확률과 정답 레이블로부터 cross entropy 오차를 가지고 손실로 사용해 학습을 진행한다.\n",
    "\n",
    "\n",
    "위 그림에서 softmax와 cross entropy error 계층만 추가된 것을 볼 수 있다. \n",
    "여기서는 softmax와 cross entropy error를 합쳐 하나의 계층으로 구현한다.   \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134809645-65c14254-cb91-43ec-b074-b83788959e68.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossentropy = log softmax + negative log likelihood\n",
    "logsoftmax -> crossen , crosse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-inventory",
   "metadata": {},
   "source": [
    "### 3.2.3 word2vec의 가중치와 분산 표현\n",
    "\n",
    "word2vec에는 두 가지 가중치를 사용한다. \n",
    "* $W_{in}$ : 각 행이 단어의 분산 표현에 해당\n",
    "* $W_{out}$ : 단어의 의미가 인코딩된 벡터가 저장. 아래 그림처럼 각 단어의 분산 표현이 열 방향으로 저장된다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/134809708-11ec5168-7caf-4fc4-a897-afe1f78488c6.png)\n",
    "\n",
    "최종적으로 이용하는 단어의 분산 표현으로는 어느 쪽 가중치를 선택하는 것이 좋을까?\n",
    "1. 입력 측의 가중치만 이용\n",
    "2. 출력 측의 가중치만 이용\n",
    "3. 양쪽 가중치 모두 이용\n",
    "\n",
    "word2vec에서는 1번인 '입력 측 가중치만 이용'이 가장 많이 쓰인다. 믾은 연구에서 입력 측 가중치 $W_{in}$만을 최종 단어의 분산 표현으로 이용한다. 따라서 여기서도 $W_{in}$을 단어의 분산표현으로 이용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-notebook",
   "metadata": {},
   "source": [
    "## 3.3 학습 데이터 준비\n",
    "\n",
    "### 3.3.1 맥락과 타깃\n",
    "word2vec 신경망의 입력 : 맥락\n",
    "라벨 데이터 : 타깃\n",
    "\n",
    "모델을 학습시켜 맥락을 입력했을 때 타깃이 출현할 확률을 높이는 것이 목표\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/135024295-9d42db40-5787-4e35-9d6a-20e8a4d7bb38.png)\n",
    "\n",
    "양끝 단어를 제외한 모든 단어들에 수행한다.    \n",
    "맥락 수는 여러개가 될 있지만, 타깃은 무조건 하나이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "editorial-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    corpus = [word_to_id[word] for word in words]\n",
    "    corpus = np.array(corpus)\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beginning-butter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodby', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "text='You say goodby and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-campus",
   "metadata": {},
   "source": [
    "corpus를 주면 맥락과 타깃을 반환하는 함수를 만든다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/135024733-0abfa285-94f2-4280-9d22-0aba97f53071.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sealed-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    target=corpus[window_size:-window_size] # 양 끝 단어를 제외한 모든 단어들에 수행하기 때문에\n",
    "    contexts = []\n",
    "    \n",
    "    # 양 끝을 제외한 모든 단어들에 수행\n",
    "    # 앞에서부터 인덱스\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs=[]\n",
    "        # window_size만큼 주변 단어를 가져오기 위한 for\n",
    "        for t in range(-window_size, window_size+1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx+t])\n",
    "            \n",
    "        contexts.append(cs)\n",
    "        \n",
    "    return np.array(contexts), np.array(target)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floppy-bunny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-plymouth",
   "metadata": {},
   "source": [
    "### 3.3.2 원핫 표현으로 변환\n",
    "contexts와 target은 여전히 단어 ID이므로 원핫 행렬로 바꿔준다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/135025952-b666d9bb-3ceb-464d-a6fc-da61fbfbb153.png)\n",
    "\n",
    "원핫 표현으로 변환햇을 때 contexts의 형태가 (6,2)->(6,2,7)로 변환된 것에 주목해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "governing-custody",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6,), (6, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape, contexts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "common-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''원핫 표현으로 변환\n",
    "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "    \n",
    "    # target\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2: # contexts\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus): \n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "finnish-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "institutional-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 7), (6, 2, 7))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape, contexts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-procurement",
   "metadata": {},
   "source": [
    "## 3.4 CBOW 모델 구현\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/135026872-005a8a99-2647-43cc-b508-a9ad8e0176d2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "plastic-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V,H).astype('f') # 임의로 크기를 작게 만들기 위해서 0.01을 곱함.-> 표준편차로 만들어짐.\n",
    "        W_out = 0.01 * np.random.randn(H,V).astype('f')\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.in_layer0 = MatMul(W_in) # 입력층 계층은 윈도우 크기만큼 만들어야 한다. \n",
    "        self.in_layer1 = MatMul(W_in) # 입력층에서 사용하는 가중치는 같아야 한다. \n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        \n",
    "        self.word_vecs = W_in\n",
    "        \n",
    "    # 순전파\n",
    "    # loss값을 반환한다.\n",
    "    # contexts를 3차원 배열이라고 가정한다. (미니배치수, 윈도우크기,원핫벡터)\n",
    "    # target은 2차원 (미니배치 수, 원핫벡터)\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:,0])\n",
    "        h1 = self.in_layer1.forward(contexts[:,1])\n",
    "        h = (h0+h1)*0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss=self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "    \n",
    "    # 역전파\n",
    "    # 'x'의 역전파는 서로를 바꿔주고, '+'는 그대로 통과시킨다.\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-criterion",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/63278762/135190894-5f7a1001-dfaa-4766-9a7f-5c88a0ba28cc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-vietnamese",
   "metadata": {},
   "source": [
    "params리스트에 같은 가중치가 여러개 존재하면 옵티마이저 처리에 영향을 주기 때문에 중복을 없애는 함수를 구현했다고 한다.\n",
    "```python\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    매개변수 배열 중 중복되는 가중치를 하나로 모아\n",
    "    그 가중치에 대응하는 기울기를 더한다.\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 가중치 공유 시\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 경사를 더함\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 가중치를 전치행렬로 공유하는 경우(weight tying)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-botswana",
   "metadata": {},
   "source": [
    "### 3.4.1 학습 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "silent-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 2 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 3 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 4 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 5 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 6 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 7 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 8 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 9 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 10 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 11 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 12 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 13 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 14 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 15 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 16 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 17 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 18 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 19 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 20 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 21 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 22 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 23 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 24 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 25 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 26 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 27 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 28 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 29 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 30 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 31 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 32 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 33 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 34 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 35 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 36 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 37 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 38 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 39 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 40 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 41 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 42 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 43 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 44 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 45 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 46 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 47 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 48 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 49 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 50 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 51 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 52 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 53 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 54 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 55 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 56 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 57 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 58 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 59 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 60 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 61 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 62 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 63 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 64 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 65 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 66 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 67 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 68 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 69 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 70 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 71 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 72 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 73 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 74 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 75 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 76 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 77 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 78 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 79 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 80 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 81 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 82 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
      "| 에폭 83 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 84 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 85 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
      "| 에폭 86 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 87 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
      "| 에폭 88 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 89 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 90 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 91 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
      "| 에폭 92 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
      "| 에폭 93 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
      "| 에폭 94 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 95 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 96 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
      "| 에폭 97 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
      "| 에폭 98 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
      "| 에폭 99 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 100 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
      "| 에폭 101 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 102 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
      "| 에폭 103 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
      "| 에폭 104 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 105 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
      "| 에폭 106 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
      "| 에폭 107 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
      "| 에폭 108 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
      "| 에폭 109 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
      "| 에폭 110 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
      "| 에폭 111 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
      "| 에폭 112 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
      "| 에폭 113 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 114 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
      "| 에폭 115 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
      "| 에폭 116 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
      "| 에폭 117 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
      "| 에폭 118 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
      "| 에폭 119 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
      "| 에폭 120 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
      "| 에폭 121 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
      "| 에폭 122 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 123 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 124 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 125 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
      "| 에폭 126 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
      "| 에폭 127 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 128 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
      "| 에폭 129 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
      "| 에폭 130 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
      "| 에폭 131 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
      "| 에폭 132 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
      "| 에폭 133 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
      "| 에폭 134 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 135 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 136 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
      "| 에폭 137 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
      "| 에폭 138 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 139 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
      "| 에폭 140 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 141 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 142 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
      "| 에폭 143 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 144 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 145 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 146 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 147 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 148 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
      "| 에폭 149 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 150 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 151 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 152 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 153 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
      "| 에폭 154 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
      "| 에폭 155 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
      "| 에폭 156 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
      "| 에폭 157 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 158 |  반복 1 / 2 | 시간 0[s] | 손실 1.40\n",
      "| 에폭 159 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 160 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
      "| 에폭 161 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 162 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
      "| 에폭 163 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
      "| 에폭 164 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
      "| 에폭 165 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
      "| 에폭 166 |  반복 1 / 2 | 시간 0[s] | 손실 1.40\n",
      "| 에폭 167 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 168 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 169 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 170 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
      "| 에폭 171 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 172 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
      "| 에폭 173 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 174 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 175 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 176 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
      "| 에폭 177 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
      "| 에폭 178 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 179 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
      "| 에폭 180 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 181 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 182 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
      "| 에폭 183 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 184 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
      "| 에폭 185 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
      "| 에폭 186 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
      "| 에폭 187 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
      "| 에폭 188 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 189 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 190 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
      "| 에폭 191 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
      "| 에폭 192 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
      "| 에폭 193 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 194 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 195 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 196 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
      "| 에폭 197 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 198 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
      "| 에폭 199 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 200 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 201 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 202 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 203 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 204 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
      "| 에폭 205 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 206 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 207 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 208 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 209 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
      "| 에폭 210 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
      "| 에폭 211 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 212 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 213 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 214 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 215 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 216 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 217 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 218 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 219 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 220 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 221 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 222 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 223 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 224 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 225 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 226 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
      "| 에폭 227 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 228 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 229 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 230 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 231 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 232 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 233 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 234 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 235 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 236 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 237 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 238 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 239 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 240 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 241 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
      "| 에폭 242 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 243 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 244 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 245 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 246 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 247 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 248 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 249 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 250 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 251 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 252 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 253 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 254 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 255 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 256 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 257 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 258 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 259 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 260 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 261 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 262 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 263 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 264 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 265 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 266 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 267 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 268 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 269 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 270 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 271 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
      "| 에폭 272 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 273 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 274 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 275 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 276 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 277 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
      "| 에폭 278 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
      "| 에폭 279 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 280 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 281 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 282 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
      "| 에폭 283 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 284 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 285 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 286 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 287 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 288 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
      "| 에폭 289 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 290 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 291 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 292 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 293 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 294 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 295 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 296 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 297 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 298 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 299 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 300 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 301 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 302 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 303 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 304 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 305 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 306 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 307 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 308 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 309 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 310 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 311 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 312 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 313 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 314 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 315 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 316 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 317 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 318 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 319 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 320 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 321 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 322 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 323 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 324 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 325 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 326 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 327 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 328 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 329 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 330 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 331 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 332 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 333 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 334 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 335 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 336 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 337 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 338 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 339 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 340 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 341 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 342 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 343 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 344 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 345 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 346 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 347 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 348 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 349 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 350 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 351 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 352 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 353 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 354 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 355 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 356 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 357 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 358 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 359 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 360 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 361 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 362 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 363 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 364 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 365 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 366 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 367 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 368 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 369 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 370 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 371 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 372 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 373 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 374 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 375 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 376 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 377 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 378 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 379 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 380 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 381 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 382 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 383 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 384 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 385 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 386 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 387 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 388 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 389 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 390 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 391 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 392 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 393 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 394 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 395 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 396 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 397 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 398 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 399 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 400 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 401 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 402 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 403 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 404 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 405 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 406 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 407 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
      "| 에폭 408 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 409 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 410 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 411 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 412 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 413 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 414 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 415 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 416 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 417 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 418 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 419 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 420 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 421 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 422 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 423 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 424 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 425 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 426 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 427 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 428 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 429 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 430 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 431 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 432 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 433 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 434 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 435 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 436 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 437 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 438 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 439 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 440 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 441 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 442 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 443 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 444 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 445 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 446 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 447 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 448 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 449 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 450 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 451 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 452 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 453 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 454 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 455 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 456 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 457 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 458 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 459 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 460 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 461 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 462 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 463 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 464 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 465 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 466 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 467 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 468 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 469 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 470 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 471 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 472 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 473 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 474 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 475 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 476 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 477 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 478 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 479 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 480 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 481 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 482 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 483 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 484 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 485 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 486 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 487 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 488 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 489 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 490 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 491 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 492 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 493 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 494 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 495 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 496 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 497 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 498 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 499 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 500 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 501 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 502 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 503 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 504 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 505 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 506 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 507 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 508 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 509 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 510 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 511 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 512 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 513 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 514 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 515 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 516 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 517 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 518 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 519 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 520 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 521 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 522 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 523 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 524 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 525 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 526 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 527 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 528 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 529 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 530 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 531 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 532 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 533 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 534 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 535 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 536 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 537 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 538 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 539 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 540 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 541 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 542 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 543 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 544 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 545 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 546 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 547 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 548 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 549 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 550 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 551 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 552 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 553 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 554 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 555 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 556 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 557 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 558 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 559 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 560 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 561 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 562 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 563 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 564 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 565 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 566 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 567 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 568 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 569 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 570 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 571 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 572 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 573 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 574 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 575 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 576 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 577 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 578 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 579 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 580 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 581 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 582 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 583 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 584 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 585 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 586 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 587 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 588 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 589 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 590 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 591 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 592 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 593 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 594 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 595 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 596 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 597 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 598 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 599 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 600 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 601 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 602 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 603 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 604 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 605 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 606 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 607 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 608 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 609 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 610 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 611 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 612 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 613 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 614 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 615 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 616 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 617 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 618 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 619 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 620 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 621 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 622 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 623 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 624 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 625 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 626 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 627 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 628 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 629 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 630 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 631 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 632 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 633 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 634 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 635 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 636 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 637 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 638 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 639 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 640 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 641 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 642 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 643 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 644 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 645 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 646 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 647 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 648 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 649 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 650 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 651 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 652 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 653 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 654 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 655 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 656 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 657 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 658 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 659 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 660 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 661 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 662 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 663 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 664 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 665 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 666 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 667 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 668 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 669 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 670 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 671 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 672 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 673 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 674 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 675 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 676 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 677 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 678 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 679 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 680 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 681 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 682 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 683 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 684 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 685 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 686 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 687 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 688 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 689 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 690 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 691 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 692 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 693 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 694 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 695 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 696 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 697 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 698 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 699 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 700 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 701 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 702 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 703 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 704 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 705 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 706 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 707 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 708 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 709 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 710 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 711 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 712 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 713 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 714 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 715 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 716 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 717 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 718 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 719 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 720 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 721 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 722 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 723 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 724 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 725 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 726 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 727 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 728 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 729 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 730 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 731 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 732 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 733 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 734 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 735 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 736 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 737 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 738 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 739 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 740 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 741 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 742 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 743 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 744 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 745 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 746 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 747 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 748 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 749 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 750 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 751 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 752 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 753 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 754 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 755 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 756 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 757 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 758 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 759 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 760 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 761 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 762 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 763 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 764 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 765 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 766 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 767 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 768 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 769 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 770 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 771 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 772 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 773 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 774 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 775 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 776 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 777 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 778 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 779 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 780 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 781 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 782 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 783 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 784 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 785 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 786 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 787 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 788 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 789 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 790 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 791 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 792 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 793 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 794 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 795 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 796 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 797 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 798 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 799 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 800 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 801 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 802 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 803 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 804 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 805 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 806 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 807 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 808 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 809 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 810 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 811 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 812 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 813 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 814 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 815 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 816 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 817 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 818 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 819 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 820 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 821 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 822 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 823 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 824 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 825 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 826 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 827 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 828 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 829 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 830 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 831 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 832 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 833 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 834 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 835 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 836 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 837 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 838 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 839 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 840 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 841 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 842 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 843 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 844 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 845 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 846 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 847 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 848 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 849 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 850 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 851 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 852 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 853 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 854 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 855 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 856 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 857 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 858 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 859 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 860 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 861 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 862 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 863 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 864 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 865 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 866 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 867 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 868 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 869 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 870 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 871 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 872 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 873 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 874 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 875 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 876 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 877 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 878 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 879 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 880 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 881 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 882 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 883 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 884 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 885 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 886 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 887 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 888 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 889 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 890 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 891 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 892 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 893 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 894 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 895 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 896 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 897 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 898 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 899 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 900 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 901 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 902 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 903 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 904 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 905 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 906 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 907 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 908 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
      "| 에폭 909 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 910 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 911 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 912 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 913 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 914 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 915 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 916 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 917 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 918 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 919 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 920 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 921 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 922 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 923 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 924 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 925 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 926 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 927 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 928 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 929 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 930 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 931 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 932 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 933 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 934 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 935 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 936 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 937 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 938 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 939 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 940 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 941 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 942 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 943 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 944 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 945 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 946 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 947 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 948 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 949 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 950 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 951 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 952 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 953 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 954 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 955 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 956 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 957 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 958 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 959 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 960 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 961 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 962 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 963 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 964 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 965 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 966 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 967 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 968 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 969 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 970 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 971 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 972 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 973 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 974 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 975 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 976 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 977 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 978 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 979 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 980 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 981 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 982 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 983 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 984 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 985 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 986 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 987 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 988 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 989 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 990 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 991 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 992 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 993 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 994 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 995 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 996 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 997 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 998 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 999 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 1000 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [-0.9049717   0.8855667  -0.91776067 -0.93416846  1.6909319 ]\n",
      "say [ 1.0968777 -1.1338617  1.1173865  1.0883389  1.2585019]\n",
      "goodbye [-1.0198039   0.99971455 -1.0223242  -0.98361105 -0.02786337]\n",
      "and [ 0.8826151  -0.66834205  0.8459068   1.0914882   1.8520579 ]\n",
      "i [-1.0340393   0.97864354 -1.0342762  -1.001588   -0.04442818]\n",
      "hello [-0.9331656   0.8978569  -0.9000251  -0.95148706  1.683233  ]\n",
      ". [ 1.0566597  -1.3008856   1.1103171   0.65401864 -0.2025193 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abstract-index",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [-0.9049717   0.8855667  -0.91776067 -0.93416846  1.6909319 ]\n",
      "say [ 1.0968777 -1.1338617  1.1173865  1.0883389  1.2585019]\n",
      "goodbye [-1.0198039   0.99971455 -1.0223242  -0.98361105 -0.02786337]\n",
      "and [ 0.8826151  -0.66834205  0.8459068   1.0914882   1.8520579 ]\n",
      "i [-1.0340393   0.97864354 -1.0342762  -1.001588   -0.04442818]\n",
      "hello [-0.9331656   0.8978569  -0.9000251  -0.95148706  1.683233  ]\n",
      ". [ 1.0566597  -1.3008856   1.1103171   0.65401864 -0.2025193 ]\n"
     ]
    }
   ],
   "source": [
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-humanity",
   "metadata": {},
   "source": [
    "가중치를 출력한 결과 단어 ID의 분산표현이 담겨있는 것을 알 수 있다.\n",
    "\n",
    "## 3.5 word2vec 보충\n",
    "\n",
    "CBOW모델을 확률관점에서 보자.\n",
    "![image](https://user-images.githubusercontent.com/63278762/135194043-d5ece680-f28e-4f94-bf60-a44656d1377f.png)\n",
    "\n",
    "$W_{t-1}$과 $W_{t+1}$이 주어졌을 때 $W_t$가 될 확률\n",
    "$$P(W_t|W_{t-1},W_{t+1})$$\n",
    "사후확률을 이용해 위와같이 쓸 수 있다.\n",
    "> 사후확률 : 관측이나 증거에 대한 조건부 확률\n",
    "\n",
    "위의 식을 이용해 CBOW의 loss 함수를 간단하게 만들 수 있다. \n",
    "![image](https://user-images.githubusercontent.com/63278762/135221096-8672c511-9995-42f8-83e9-0c67b2195025.png)\n",
    "\n",
    "이를 **음의 로그 가능도**라고 부른다. 이것을 말뭉치 전체로 확장하면 다음의 식이 된다.\n",
    "![image](https://user-images.githubusercontent.com/63278762/135221465-b2a0aae9-3dda-461d-9ece-6f9ed67950a5.png)\n",
    "\n",
    "### 3.5.2 skip-gram 모델\n",
    "word2vec은 2개의 모델을 제안한다.\n",
    "1. CBOW\n",
    "2. skip-gram\n",
    "\n",
    "여기서 skip-gram은 CBOW에서 다루는 contexts와 target을 역전시킨 모델이다. \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/135222442-c06b94d0-d1f8-4a5c-adc5-1c63f349af4f.png)\n",
    "\n",
    "중앙 단어(target)으로부터 주변 여러단어(contexts)를 추측\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/135223056-d772bfe9-ffac-458a-8059-324fad3e2e47.png)\n",
    "\n",
    "입력층이 하나이고 출력층은 contexts의 수만큼 존재한다. 각 출력층에서는 각각의 Loss를 먼저 구하고, 구해진 Loss들을 모두 더한 값이 최종 Loss가 된다.    \n",
    "skip-gram 모델을 확률로 표기해보면 \n",
    "![image](https://user-images.githubusercontent.com/63278762/135223286-35a3d917-1809-408d-b3b7-7b167cfee071.png)\n",
    "\n",
    "$W_t$가 일어났을 때 $W_{t-1}$, $W_{t+1}$이 동시에 일어날 확률.    \n",
    "skip-gram은 contexts들 사이 관련성이 없다고 가정하고 분해를 한다.(조건부 독립)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/135223534-415a590f-8c15-4f1e-a66f-b52bc15fc5ae.png)\n",
    "\n",
    "교차 엔트로피 오차를 적용하면 \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63278762/135224116-8d469a82-1008-4f00-b713-8d73baad2a04.png)\n",
    "\n",
    "말뭉치 전체로 확장한 손실함수\n",
    "![image](https://user-images.githubusercontent.com/63278762/135224138-8b8988c7-237c-4f68-a959-00ee7cb556ff.png)\n",
    "\n",
    "\n",
    "* CBOW 모델은 타깃 하나의 손실을 구하고, skip-gram은 각 맥락에서 구한 손실의 총합을 구한다.\n",
    "\n",
    "##### 그렇다면 CBOW와 skip-gram 중 어떤 것을 사용해야 할까?\n",
    "여기서는 skip-gram 모델의 결과가 더 좋은 경우가 많다. 특히 말뭉치가 커질수록 저빈도 단어나 유추 문제의 성능 면에서 skip-gram 모델이 더 뛰어나다.    \n",
    "반면, 속도 면에서는 CBOW 모델이 더 빠르다. \n",
    "\n",
    "=> skip-gram은 기울기랑 loss도 두개로 계산. 더 어려운 모델이라 오래걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "rural-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "\n",
    "class SimpleSkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer1 = SoftmaxWithLoss()\n",
    "        self.loss_layer2 = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "        s = self.out_layer.forward(h)\n",
    "        l1 = self.loss_layer1.forward(s, contexts[:, 0])\n",
    "        l2 = self.loss_layer2.forward(s, contexts[:, 1])\n",
    "        loss = l1 + l2\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dl1 = self.loss_layer1.backward(dout)\n",
    "        dl2 = self.loss_layer2.backward(dout)\n",
    "        ds = dl1 + dl2\n",
    "        dh = self.out_layer.backward(ds)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-asthma",
   "metadata": {},
   "source": [
    "### 3.5.3 통계 기반 vs. 추론 기반\n",
    "#### 통계기반\n",
    "말뭉치 전체 통계로부터 1회 학습해 단어의 분산 표현을 얻음.\n",
    "* 새로운 단어를 추가할 때 : 처음부터 다시 계산해야 한다.\n",
    "* 단어의 유사성이 인코딩된다.\n",
    "\n",
    "#### 추론기반\n",
    "말뭉치를 조금씩 여러번 보며 학습(미니배치)\n",
    "* 새로운 단어를 추가할 때 : 기존에 학습한 가중치를 초깃값으로 다시 학습시킨다.\n",
    "* 단어의 유사성과 더 복잡한 단어 사이 패턴도 인코딩 된다.\n",
    "\n",
    "\n",
    "하지만 유사성을 평가했을 때 통계기반과 추론기반의 우열을 가릴 수 없다.    \n",
    "또, 추론 기반과 통계 기반은 서로 관련되어 있다. skip-gram과 네거티브 샘플링을 이용한 모델은 모두 말뭉치 전체의 동시발생 행렬에 특수한 행렬 분해를 적용한 것과 같다. \n",
    "\n",
    "word2vec 이후 추론 기반과 통계 기반을 융합한 GloVe 기법이 등장했다.    \n",
    "기본 아이디어는 말뭉치 전체의 통계 정보를 손실 함수에 도입해 미니배치 학습을 하는 것이다.\n",
    "\n",
    "\n",
    "\n",
    "## Summary\n",
    "* 추론 기반 기법은 추측을 해서 단어의 분산 표현을 얻는 것이다.\n",
    "* word2vec은 추론 기반 기법이고 단순한 2층 신경망이다.\n",
    "* word2vec은 skip-gram 모델과 CBOW 모델을 제공한다.\n",
    "* CBOW 모델은 여러 단어로부터 하나의 단어를 추측한다.\n",
    "* skip-gram 모델은 하나의 단어로부터 여러 단어를 추측한다.\n",
    "* word2vec은 가중치를 다시 학습할 수 있으므로, 단어의 분산 표현 갱신이나 새로운 단어 추가를 효율적으로 수행할 수 있다. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
